Overall good progress. Some comments on the challenges part: we understand that some classes are similar and hard to distinguish (even for humans), so it's OK if the model cannot classify them very well. But if it behaves poorly overall, you might want to increase/clean your data, finetune your model etc. Regarding training time, the easiest thing you can consider is to incrase the batchsize (increase the parallelisim). You can keep increasing the batchsize until the model cannot fit in GPU memory(You can check GPU memory utilization using the command "nvidia-smi"). I am not sure the size of your increase dataset. I think the training time would be reasonable if your dataset is on the order of 10k to 100k images given max utilization of a decent GPU (like K80 in gcp). Regarding virtual env in gcp. I didn't check carefully, but I image they should definitely support that, so you don't need to worry too much about that. Regarding API with missing content, you could consider if there are any backup plans (like other API). We cannot do much about the request limit, just use them carefully and maybe finding other API could help.
